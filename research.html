<!DOCTYPE html>
<!--
	Landed by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <title>Gurpreet - Research</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript
      ><link rel="stylesheet" href="assets/css/noscript.css"
    /></noscript>
  </head>
  <body class="is-preload">
    <div id="page-wrapper">
      <!-- Header -->
      <header id="header">
        <h1 id="logo"><a href="index.html">Home</a></h1>
        <nav id="nav">
          <ul>
            <li><a href="research.html">Research</a></li>
            <li><a href="publication.html">Publication</a></li>
            <li><a href="">Portfolio</a></li>
            <li><a href="docs/gurpreet_CV.pdf" class="button primary" target="_blank">CV</a></li>
          </ul>
        </nav>
      </header>

      <!-- Main -->
      <div id="main" class="wrapper style1">
        <div class="container">
          <header class="major">
            <h2>Research</h2>
            <p>
              Bi-orthogonality and Representations: Interpretable Neural Networks for Feature/Kernel Extraction
            </p>
            <a href="#" class="image fit"
              ><img src="images/Interpretable.png" alt=""  style="width:1400px;height:425px;"
            /></a>
          </header>
          <header class="major">
            <section>
              <div class="row">
                <div class="col-1"></div>
                <div class="col-10">
                  <ul class="actions fit">
                    <p>My research focusses on developing Interpretable Neural Network models where the learned representations satisfy 
                      a bi-orthogonality property. The features are then extracted as vectors of this learned representation once the network minimization
                       problem converges. A common misconception is that accuracy (approximation errors) deteriorate as we attempt 
                       to enforce interpretability. My work demonstrates that, with bi-orthogonality bothaccuracy and interpretability increases in tandem. 
                       Additionally, identifying and exploiting the orthogonal structure in any dataset allows one to avoid fail-safes against over-fitting such 
                       as kernel regularization, dropout, all of which come with an additional burden of hyper-parameter/penalty tuning. For practical applications, 
                       it is not known a priori whether the assumptions of a model formulation are satisfied. As a consequence, a trained network can suffer not only
                       from approximation errors but model errors as well. My information preserving formulations explore the idea that extracted features span
                      an orthogonal vector space (orthogonality under extracted weights) unique to the dataset and therefore assumptions on a model formulation must be 
                      kept minimal unless explicitly known otherwise.
                       
                    </p>
                  </ul>
                  <ul class="actions fit">
                    <li><a href="#nets" class="button fit">Neural Nets</a></li>
                    <li><a href="#ml" class="button fit">Porous Media Applications</a></li>
                    <li><a href="#ir" class="button fit">Bio-informatics</a></li>
                  </ul>
                </div>
                <div class="col-1"></div>
              </div>
            </section>
          </header>
          
          <header style="text-align:center" id="nets">
            <h2>Neural Nets</h2>
          </header>
          
          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a class="image fit"><img src="images/hsi.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>HyperSpectral Unmixing for Linear and Non-Linear Mixture Models</h3>
                  <p>Coming Soon.</p>
                </section>
            </div>
          </div>

          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a class="image fit"><img src="images/pareto.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Pareto Optimal Solutions for Comprehensive Neural Network Training</h3>
                  <p> Neural networks have found applications in several domains requiring that the training and tuning process 
                    be delineated completely for reproduciblity. However, the network minimization problem or loss function often 
                    contains multiple user specified constraints and objectives that must be satisfied for accurate feature extraction.
                    For example, a user might specify a domain-informed constraint or requirement giving rise to a multi-objective problem. 
                    This constraint now shows up as an additive term in the loss function where its accompanying hyper-parameter requires 
                    tuning without any prior knowledge. Pareto optimal solutions for these problems is a set of network weights and hyper-parameters
                    for optimal feature extraction. With our two-stage hybrid neural approach for Pareto set extraction we attempt to do away with 
                    reporting means and variances in the network training report over multiple runs replacing the learned network states as Pareto sets.
                    This also allows us to generate a comprehensive training report where all optimal network representations and hyper-parameter values 
                    are represented as elements of this Pareto set that can be easily visualized by the user to aid decision making.
                </section>
            </div>
          </div>

          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a class="image fit"><img src="images/svd.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Streaming Singular Value Decomposition for Big Data</h3>
                  <p>
                    Singular Value Decomposition in a Big Data setting is often restrictive due to the main memory requirements imposed 
                    by the dataset. We present a two stage neural optimization approach where the memory requirement depends explicitly 
                    on the feature dimension and desired rank, independent of the sample size. The proposed scheme reads data samples 
                    in a streaming setting with the network minimization problem converging to a low rank approximation with high precision. 
                    Our architecture is fully interpretable where all the network outputs and weights have a specific meaning. Additionally, solution 
                    achieves tail-energy bounds at machine precision for an accurate extraction of singular vectors.
                  </p>
                </section>
            </div>
          </div>

          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a class="image fit"><img src="images/manifold.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Intersecting Manifold Detection for Non Convex Fucntions</h3>
                  <p>
                    Constrained Optimization solution algorithms are restricted to point based solutions. Single or 
                    multiple objectives must be satisfied, wherein both the objective function and constraints can be 
                    non-convex resulting in multiple optimal solutions. Scenarios include intersecting surfaces 
                    as Implicit Functions. We present neural solutions for extracting optimal sets as 
                    approximate manifolds, where non-convex objectives and constraints are defined as modeler 
                    guided, domain-informed L2 loss function. This promotes interpretability 
                    since modelers can confirm the results against known analytical forms.
                  </p>
                </section>
            </div>
          </div>

          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a class="image fit"><img src="images/time.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>CNN for Identifying Kernels of Physical Process</h3>
                  <p> 
                    We present a fully convolutional architecture that captures the invariant structure of the domain to 
                    reconstruct the observable system, defined by Partial Differential Equations. Our intent is to learn coupled 
                    dynamic processes interpreted as deviations from true kernels representing isolated processes. The architecture 
                    is robust and transparent in capturing process kernels and system anomalies. We also show that high weights 
                    representation is not only redundant but also impacts network interpretability. These allow us to identify 
                    redundant kernels and their manifestations in activation maps to guide better designs.
                  </p>
                </section>
            </div>
          </div>

          <header class="major"></header>
<!-- 
          <header style="text-align:center" id="ml">
            <h2>Machine Learning</h2>
          </header>
          
          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a href="#" class="image fit"><img src="images/hofs.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Higher Order Feature Seletion</h3>
                  <p>
                    Feature selection is a process of choosing a subset of relevant features so that the quality of prediction models 
                    can be improved. We present a higher-order MI based approximation technique called Higher OrderFeature Selection (HOFS). 
                    Instead of producing a single list of features, our method produces a ranked collection of feature subsets that 
                    maximizes MI, giving better comprehension (feature ranking) as to which features work best together when selected, 
                    due to their underlying interdependent structure. 
                  </p>
                </section>
            </div>
          </div>

          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a href="#" class="image fit"><img src="images/mnf.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Minimum Noise Fraction based Denoising of HyperSpectral Images</h3>
                  <p>
                    Minimum Noise Fraction (MNF) developed by Green et al. has been extensively studied as a method for noise removal 
                    in HSI data. However, it entails a manual speed-accuracy trade-off, namely the process of manually selecting the 
                    relevant bands in the MNF space. We present a low-rank model where the computational time of the algorithm is reduced, 
                    as well as the entire process of band selection is fully automated. This automated approximations produced by the 
                    algorithm show the reconstruction accuracy vs storage (50×) and runtime speed (60×) trade-off.
                  </p>
                </section>
            </div>
          </div>

          <header class="major"></header>

          <header style="text-align:center" id="ir">
            <h2>Information Retrieval</h2>
          </header>
          
          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a href="#" class="image fit"><img src="images/metric.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Ranking of IR metrics based on Information Content</h3>
                  <p>
                    Given limited time and space, IR studies report few evaluation metrics which must be carefully selected. We quantify 
                    correlation between popular IR metrics on TREC test collections. Next, we investigate prediction of unreported metrics: 
                    given 1-3 metrics, we assess the best predictors for 10 others. We further explore whether high-cost evaluation measures 
                    can be predicted using low-cost measures. We also present a novel model for ranking evaluation metrics based on 
                    covariance, enabling selection of a set of metrics that are most informative and distinctive.
                  </p>
                </section>
            </div>
          </div>

        </div>
      </div> -->

      <button id="btn"><a href="#main"><i class="fa fa-angle-up"></i></a></button>


      <!-- Footer -->
      <footer id="footer">
        <ul class="icons">
          <li>
            <a
              href="https://github.com/grpt-singh"
              target="_blank"
              class="icon brands alt fa-github"
              ><span class="label">GitHub</span></a
            >
          </li>
          <li>
            <a
              href="https://www.linkedin.com/in/grpt-singh/"
              target="_blank"
              class="icon brands alt fa-linkedin-in"
              ><span class="label">LinkedIn</span></a
            >
          </li>
          <li>
            <a
              href="https://www.researchgate.net/profile/Gurpreet_Singh82"
              target="_blank"
              class="icon brands alt fa-researchgate"
              ><span class="label">Research Gate</span></a
            >
          </li>
        </ul>
        <ul class="copyright">
          <li>&copy; Gurpreet@2021. All rights reserved.</li>
          <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
        </ul>
      </footer>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/jquery.dropotron.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
