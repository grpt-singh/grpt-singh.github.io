<!DOCTYPE html>
<!--
	Landed by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <title>Gurpreet - Research</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript
      ><link rel="stylesheet" href="assets/css/noscript.css"
    /></noscript>
  </head>
  <body class="is-preload">
    <div id="page-wrapper">
      <!-- Header -->
      <header id="header">
        <h1 id="logo"><a href="index.html">Home</a></h1>
        <nav id="nav">
          <ul>
            <li><a href="research.html">Research</a></li>
            <li><a href="publication.html">Publication</a></li>
            <li><a href="">Portfolio</a></li>
            <li><a href="docs/gurpreet_CV.pdf" class="button primary" target="_blank">CV</a></li>
          </ul>
        </nav>
      </header>

      <!-- Main -->
      <div id="main" class="wrapper style1">
        <div class="container">
          <header class="major">
            <h2>Research</h2>
            <p>
              Bi-orthogonality and Representations: Interpretable Neural Networks for Feature/Kernel Extraction
            </p>
            <a href="#" class="image fit"
              ><img src="images/Interpretable.png" alt=""  style="width:1400px;height:425px;"
            /></a>
          </header>
          <header class="major">
            <section>
              <div class="row">
                <div class="col-1"></div>
                <div class="col-10">
                  <ul class="actions fit">
                    <p>My research focusses on developing Interpretable Neural Network models where the learned representations are bi-orthogonal. The features are then extracted as vectors of this learned representation once the network minimization
                       problem converges. A common misconception is that accuracy (approximation errors) deteriorate as we attempt 
                       to enforce interpretability. My work demonstrates that, with bi-orthogonality both accuracy and interpretability increases in tandem. 
                       Additionally, identifying and exploiting the orthogonal structure in any dataset allows one to avoid fail-safes against over-fitting such 
                       as kernel regularization, dropout, all of which come with an additional burden of hyper-parameter/penalty tuning. For practical applications, 
                       it is not known a priori whether the assumptions of a model formulation are satisfied. As a consequence, a trained network can suffer not only
                       from approximation errors but model errors as well. My information preserving formulations explore the idea that extracted features span
                      an orthogonal vector space (orthogonality under extracted weights) unique to the dataset and therefore assumptions on a model formulation must be 
                      kept minimal unless explicitly known otherwise.
                       
                    </p>
                  </ul>
                  <ul class="actions fit">
                    <li><a href="#nets" class="button fit">Neural Nets</a></li>
                    <li><a href="#ml" class="button fit">Porous Media Applications</a></li>
                    <li><a href="#ir" class="button fit">Bio-informatics</a></li>
                  </ul>
                </div>
                <div class="col-1"></div>
              </div>
            </section>
          </header>
          
          <header style="text-align:center" id="nets">
            <h2>Neural Nets</h2>
          </header>

          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a class="image fit"><img src="images/pic02.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>RAE: A Reverse AutoEncoder for Feature Classification</h3>
                  <p>Coming soon. In this work, we show that a classification problem is a partition of unity of an existing space spanned by a
                    bi-orthogonal representation. We verify and benchmark our results against the well known k-means (first-order momemnt extraction)
                    and extend the framework to encompass Gaussian Mixture Model (mean and variance) with the intent to extract higher order moments 
                    (skewness, kurtosis) of a given dataset in our future work.
                  </p>
                </section>
            </div>
          </div>
          
          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a class="image fit"><img src="images/hsi.png" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>SCA-Net: A Self-Correcting Autoencoder for Hyperspectral Unmixing</h3>
                  <p>Linear Mixture Model for hyperspectral datasets involves separating a mixed pixel as a linear combination of its constituent endmembers and corresponding fractional abundances.
                    Both optimization and neural methods have attempted to tackle this problem, with the current state of the art results achieved by neural models on benchmark datasets. 
                    Our work shows for the first time that a two-layer autoencoder (SCA-Net), with 2FK parameters (F features, K endmembers), achieves error metrics that are scales apart (1.E-5) 
                    from previously reported values around 1.E-2. SCA-Net converges to this low error solution starting from a random initialization of weights. We also show that SCA-Net, based upon a 
                    bi-orthogonal representation, performs a self-correction when the the number of endmembers are over-specified. Our network formulation extracts a low-rank representation that is bounded 
                    below by a tail-energy and can be computationally verified. Our numerical experiments on Samson, Jasper, and Urban datasets demonstrate that SCA-Net outperforms previously 
                    reported error metrics for all the cases while being robust to noise and outliers.
                  </p>
                </section>
            </div>
          </div>

          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a class="image fit"><img src="images/pareto.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Pareto Optimal Solutions for Comprehensive Neural Network Training</h3>
                  <p> Neural networks have found applications in several domains requiring that the training and tuning process 
                    be delineated completely for reproduciblity. However, the network minimization problem or loss function often 
                    contains multiple user specified constraints and objectives that must be satisfied for accurate feature extraction.
                    For example, a user might specify a domain-informed constraint or requirement giving rise to a multi-objective problem. 
                    This constraint now shows up as an additive term in the loss function where its accompanying hyper-parameter requires 
                    tuning without any prior knowledge. Pareto optimal solutions for these problems is a set of network weights and hyper-parameters
                    for optimal feature extraction. With our two-stage hybrid neural approach for Pareto set extraction we attempt to do away with 
                    reporting means and variances in the network training report over multiple runs replacing the learned network states as Pareto sets.
                    This also allows us to generate a comprehensive training report where all optimal network representations and hyper-parameter values 
                    are represented as elements of this Pareto set that can be easily visualized by the user to aid decision making.
                </section>
            </div>
          </div>

          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a class="image fit"><img src="images/svd.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Streaming Singular Value Decomposition for Big Data Applications</h3>
                  <p>
                    Singular Value Decomposition in a Big Data setting is often restrictive due to the main memory requirements imposed 
                    by the dataset. We present a two stage neural optimization approach where the memory requirement depends explicitly 
                    on the feature dimension and desired rank, independent of the sample size. The proposed scheme reads data samples 
                    in a streaming setting with the network minimization problem converging to a low rank approximation with high precision. 
                    Our architecture is fully interpretable where all the network outputs and weights have a specific meaning. Additionally, solution 
                    achieves tail-energy bounds at machine precision for an accurate extraction of singular vectors.
                  </p>
                </section>
            </div>
          </div>

          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a class="image fit"><img src="images/manifold.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Intersecting Manifold Detection for Non Convex Fucntions</h3>
                  <p>
                    Constrained Optimization solution algorithms are restricted to point based solutions. Single or 
                    multiple objectives must be satisfied, wherein both the objective function and constraints can be 
                    non-convex resulting in multiple optimal solutions. Scenarios include intersecting surfaces 
                    as Implicit Functions. We present neural solutions for extracting optimal sets as 
                    approximate manifolds, where non-convex objectives and constraints are defined as modeler 
                    guided, domain-informed L2 loss function. This promotes interpretability 
                    since modelers can confirm the results against known analytical forms.
                  </p>
                </section>
            </div>
          </div>

          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a class="image fit"><img src="images/time.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>CNN for Identifying Kernels of a Physical Process</h3>
                  <p> 
                    We present a fully convolutional architecture that captures the invariant structure of the domain to 
                    reconstruct the observable system, defined by Partial Differential Equations. Our intent is to learn coupled 
                    dynamic processes interpreted as deviations from true kernels representing isolated processes. The architecture 
                    is robust and transparent in capturing process kernels and system anomalies. We also show that high weights 
                    representation is not only redundant but also impacts network interpretability. These allow us to identify 
                    redundant kernels and their manifestations in activation maps to guide better designs.
                  </p>
                </section>
            </div>
          </div>

          <header class="major"></header>
<!-- 
          <header style="text-align:center" id="ml">
            <h2>Machine Learning</h2>
          </header>
          
          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a href="#" class="image fit"><img src="images/hofs.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Higher Order Feature Seletion</h3>
                  <p>
                    Feature selection is a process of choosing a subset of relevant features so that the quality of prediction models 
                    can be improved. We present a higher-order MI based approximation technique called Higher OrderFeature Selection (HOFS). 
                    Instead of producing a single list of features, our method produces a ranked collection of feature subsets that 
                    maximizes MI, giving better comprehension (feature ranking) as to which features work best together when selected, 
                    due to their underlying interdependent structure. 
                  </p>
                </section>
            </div>
          </div>

          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a href="#" class="image fit"><img src="images/mnf.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Minimum Noise Fraction based Denoising of HyperSpectral Images</h3>
                  <p>
                    Minimum Noise Fraction (MNF) developed by Green et al. has been extensively studied as a method for noise removal 
                    in HSI data. However, it entails a manual speed-accuracy trade-off, namely the process of manually selecting the 
                    relevant bands in the MNF space. We present a low-rank model where the computational time of the algorithm is reduced, 
                    as well as the entire process of band selection is fully automated. This automated approximations produced by the 
                    algorithm show the reconstruction accuracy vs storage (50×) and runtime speed (60×) trade-off.
                  </p>
                </section>
            </div>
          </div>

          <header class="major"></header>

          <header style="text-align:center" id="ir">
            <h2>Information Retrieval</h2>
          </header>
          
          <div class="row gtr-150">
            <div class="col-4 col-12-medium">
                <section id="sidebar">
                  <section>
                    <a href="#" class="image fit"><img src="images/metric.jpg" alt="" /></a>
                  </section>
                </section>
            </div>
            <div class="col-8 col-12-medium imp-medium">
                <section id="content">
                  <h3>Ranking of IR metrics based on Information Content</h3>
                  <p>
                    Given limited time and space, IR studies report few evaluation metrics which must be carefully selected. We quantify 
                    correlation between popular IR metrics on TREC test collections. Next, we investigate prediction of unreported metrics: 
                    given 1-3 metrics, we assess the best predictors for 10 others. We further explore whether high-cost evaluation measures 
                    can be predicted using low-cost measures. We also present a novel model for ranking evaluation metrics based on 
                    covariance, enabling selection of a set of metrics that are most informative and distinctive.
                  </p>
                </section>
            </div>
          </div>

        </div>
      </div> -->

      <button id="btn"><a href="#main"><i class="fa fa-angle-up"></i></a></button>


      <!-- Footer -->
      <footer id="footer">
        <ul class="icons">
          <li>
            <a
              href="https://github.com/grpt-singh"
              target="_blank"
              class="icon brands alt fa-github"
              ><span class="label">GitHub</span></a
            >
          </li>
          <li>
            <a
              href="https://www.linkedin.com/in/grpt-singh/"
              target="_blank"
              class="icon brands alt fa-linkedin-in"
              ><span class="label">LinkedIn</span></a
            >
          </li>
          <li>
            <a
              href="https://www.researchgate.net/profile/Gurpreet_Singh82"
              target="_blank"
              class="icon brands alt fa-researchgate"
              ><span class="label">Research Gate</span></a
            >
          </li>
        </ul>
        <ul class="copyright">
          <li>&copy; Gurpreet@2021. All rights reserved.</li>
          <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
        </ul>
      </footer>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/jquery.dropotron.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
